<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>sanketsans.github.io</title>
    <description>Peronal website</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Fri, 12 May 2023 11:27:53 +0200</pubDate>
    <lastBuildDate>Fri, 12 May 2023 11:27:53 +0200</lastBuildDate>
    <generator>Jekyll v3.9.0</generator>
    
        
          <item>
            <title>Who is Looking ?</title>
            <description>&lt;h2 id=&quot;what-&quot;&gt;What ?&lt;/h2&gt;

&lt;p&gt;Identifying and segmenting the person  seen from multiple view, also the person whose first person view is available.&lt;/p&gt;

&lt;h2 id=&quot;why-&quot;&gt;Why ?&lt;/h2&gt;

&lt;p&gt;Combining multi camera views and having a reconstruct the events.&lt;/p&gt;

&lt;h2 id=&quot;how-&quot;&gt;How ?&lt;/h2&gt;
&lt;p&gt;All the videos are egocentric in nature**&lt;/p&gt;

&lt;p&gt;Establish a separate segmentation module to identify the segmentation mask given a pre-mask(prev. segmentation mask) for specific / target person, using RGB and optical flow.
A joint network is then created for the given task&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Given two ‘third-third’ person views:&lt;/em&gt;&lt;/strong&gt;
Identify the common person viewed from both the views. The joint network is constructed using two segmentation module and the mask is then fed to another set of CNN with shared weights to segment the common person.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Given one third person (egocentric) view and one first person view:&lt;/em&gt;&lt;/strong&gt;
Identify the person whose first person view is available. The joint network is composed of one segmentation module for thirs person view and first person view is fed to VGG module. The mask from both module is then fed to another CNN. The weights are only shared for embedding space. The CNN features from VGG spatial is multiplied to soft attention of background and VGG temporal features with soft attention of foreground since person in FPV himself does not appear but background might reflect some similarities.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Loss:&lt;/em&gt;&lt;/strong&gt;
Authors propose using two loss function : sigmoid cross entropy ( to correct the segmentation mask ) and constrastive loss ( to encourage less distances b/w positive samples and more for negative samples. )&lt;/p&gt;

&lt;h3 id=&quot;limitations&quot;&gt;Limitations&lt;/h3&gt;

&lt;p&gt;Only performed for egocentric videos(IUShareView dataset). The model always assumes atleast one person in each frame hence does not perform well for no-person frames.&lt;/p&gt;

&lt;h3 id=&quot;tldr-&quot;&gt;TL/DR :&lt;/h3&gt;
&lt;p&gt;Evaluation protocol&lt;/p&gt;
</description>
            <pubDate>Mon, 13 Sep 2021 00:00:00 +0200</pubDate>
            <link>http://localhost:4000/blog/whoislooking</link>
            <guid isPermaLink="true">http://localhost:4000/blog/whoislooking</guid>
            
            <category>egocentric</category>
            
            
            <category>egocentric</category>
            
          </item>
        
    
        
          <item>
            <title>Music Separation</title>
            <description>&lt;h3 id=&quot;what&quot;&gt;What&lt;/h3&gt;
&lt;p&gt;Visual Sound separation for different music instruments.&lt;/p&gt;

&lt;h3 id=&quot;why&quot;&gt;Why&lt;/h3&gt;
&lt;p&gt;Existing methods rely on appearance (also motion (optical flow)) based methods but do explore the relationship between audio and body keypoints.&lt;/p&gt;

&lt;h3 id=&quot;how&quot;&gt;How&lt;/h3&gt;
&lt;p&gt;Compute keypoints for three video music datasets [ Music-21, URMP, AtinPiano ]. Main motivation is to associate body dynamics with audio signals for sound keypoints. Extracts global context features from video frames and merge with the outputs of context-aware GCNN on human keypoints. Each input to G-CNN is 2D keypoint coordinate.
For audio, mixture spectograms are fed to an encoder-decoder network, where are the (visual + GCNN) are fused with encoder features with a self-guided attention mechanism. The U-net is responsible to generate binary spectogram mask for individual audio which is compared with a ground truth mask for dominant component in input mixed audio.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;visual features -&amp;gt; (2048, D_a x 1) D_a - dimensions
graph features -&amp;gt; (N x T x D_p)

v = visual network fusion -&amp;gt; N x T x (D_a + D_p) ~ V x D_v

a = audio features -&amp;gt; F x D_s

## Self-Guided Attention :

h_t = np.dot(SOFTMAX(np.dot(v, a)), v) + a

## Fused Features :

z = MLP(h_t) + h_t
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The MLP is simply a two fully connected layer. The fused features are then fed to upsample layers of decoder part of the U-net. To create a binary mask, sigmoid operation is performed on the feature map.&lt;/p&gt;

&lt;p&gt;The network is trained by minimising the per-pixel sigmoid cross-entropy loss.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;TL;DR&lt;/em&gt;&lt;/strong&gt; Reconstruct waveforms from spectograms.&lt;/p&gt;

&lt;h3 id=&quot;notes&quot;&gt;Notes&lt;/h3&gt;
&lt;p&gt;Keypoint based representation only can perform better than RGB + Keypoints.&lt;/p&gt;
</description>
            <pubDate>Fri, 20 Aug 2021 00:00:00 +0200</pubDate>
            <link>http://localhost:4000/blog/music-separation</link>
            <guid isPermaLink="true">http://localhost:4000/blog/music-separation</guid>
            
            <category>sound-separation</category>
            
            
            <category>sound_separation</category>
            
          </item>
        
    
        
          <item>
            <title>3D vs 2D CNN</title>
            <description>&lt;h3 id=&quot;what&quot;&gt;What&lt;/h3&gt;
&lt;p&gt;Comparing different Spatiotemporal CNN model for action recognition task and proposes a new spatiotemporal CNN block : R(2+1)D CNN&lt;/p&gt;

&lt;h3 id=&quot;why&quot;&gt;Why&lt;/h3&gt;
&lt;p&gt;2D CNN applied to individual frames in a video gives appreciable results even after discarding temporal features.&lt;/p&gt;

&lt;h3 id=&quot;how&quot;&gt;How&lt;/h3&gt;
&lt;p&gt;The authors proposes multiple 3D CNN (reversed, mixed) and CNN with similar dimensions and validate with different clip size for accuracy performance comparison.&lt;/p&gt;

&lt;p&gt;They also argue that since 3D CNN store the temporal information for the clip length, it tends to perform better than 2D CNN .&lt;/p&gt;

&lt;p&gt;It is also proved that since R(2+1)D has more non-linearity (due to additional ReLU with 2D and 1d convolutional block), it has the best performance compared to other CNN models.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;TL;DR&lt;/em&gt;&lt;/strong&gt; Action recognition with a 34- layer R(2+1)D net&lt;/p&gt;

&lt;h3 id=&quot;notes&quot;&gt;Notes&lt;/h3&gt;
&lt;p&gt;Training on longer clips yields better results on clip-level models, as the filter learn long-term temporal dimensions,&lt;/p&gt;
</description>
            <pubDate>Sat, 26 Jun 2021 00:00:00 +0200</pubDate>
            <link>http://localhost:4000/blog/cnn-compare</link>
            <guid isPermaLink="true">http://localhost:4000/blog/cnn-compare</guid>
            
            <category>action-recognition</category>
            
            
            <category>action_recognition</category>
            
          </item>
        
    
        
          <item>
            <title>FACExplanations</title>
            <description>&lt;h2 id=&quot;what-&quot;&gt;What ?&lt;/h2&gt;

&lt;p&gt;Feasible and Actionable counterfactuals using high density paths of data points and custom weight functions.&lt;/p&gt;

&lt;h2 id=&quot;why-&quot;&gt;Why ?&lt;/h2&gt;

&lt;p&gt;Counterfactuals does not consider underlying data distributions and constraints that makes them unachievable and infeasible.&lt;/p&gt;

&lt;h2 id=&quot;how-&quot;&gt;How ?&lt;/h2&gt;

&lt;p&gt;A graph over the data points is constructed where edges are weighted using different metrics based on distances and density of data points. This metric can be tuned based on properties of target instance (to be explained counterfactually) - prediction threshold (for distance) and density threshold.&lt;/p&gt;

&lt;div align=&quot;center&quot; class=&quot;img-container&quot; style=&quot;margin-top:2%&quot;&gt;

    &lt;img src=&quot;/img/FACE/counter_path.png&quot; width=&quot;80%&quot; height=&quot;60%&quot; border=&quot;1px&quot; alt=&quot;arch&quot; class=&quot;center&quot; /&gt;
    &lt;figcaption&gt;HPGD&lt;/figcaption&gt;
&lt;/div&gt;

&lt;p&gt;For the given data points graph, FACE suggests D to be the best counterfactual. As A has low classification margin and is in low-density region. B has good classification margin but again is in low-density. C is in high density but is not connected by a high density path.&lt;/p&gt;

&lt;div align=&quot;center&quot; class=&quot;img-container&quot; style=&quot;margin-top:2%&quot;&gt;

    &lt;img src=&quot;/img/FACE/epsilon_low.png&quot; width=&quot;80%&quot; height=&quot;60%&quot; border=&quot;1px&quot; alt=&quot;arch&quot; class=&quot;center&quot; /&gt;
    &lt;figcaption&gt;Low threshold&lt;/figcaption&gt;
&lt;/div&gt;

&lt;p&gt;The counterfactuals generated ensures a path that guarantees feasible changes (due to high density data points). Furthermore, a custom weight and condition function is provided to remove inappropriate edges. The figure compares the differences in constructing counterfactuals by typical KDE (Kernel Density Estimator) approach (Left) and FACE approach (Right).&lt;/p&gt;

&lt;div align=&quot;center&quot; class=&quot;img-container&quot; style=&quot;margin-top:2%&quot;&gt;

    &lt;img src=&quot;/img/FACE/epsilon_high.png&quot; width=&quot;80%&quot; height=&quot;60%&quot; border=&quot;1px&quot; alt=&quot;arch&quot; class=&quot;center&quot; /&gt;
    &lt;figcaption&gt;High threshold&lt;/figcaption&gt;
&lt;/div&gt;

&lt;p&gt;However, the paths can also be decided based on a distance threshold metric, &lt;i&gt;ε&lt;/i&gt; , when the user wants to somewhat ignore the density metric. Different use cases can use different density thresholds based on the distribution of the data.&lt;/p&gt;

&lt;h3 id=&quot;notes&quot;&gt;Notes&lt;/h3&gt;
&lt;p&gt;Assumes covariates to be distributed only on the basis of density. Actionable counterfactuals could be generated based on input constraints as well.&lt;/p&gt;
</description>
            <pubDate>Fri, 25 Jun 2021 00:00:00 +0200</pubDate>
            <link>http://localhost:4000/blog/facexplanations</link>
            <guid isPermaLink="true">http://localhost:4000/blog/facexplanations</guid>
            
            <category>XAI</category>
            
            
            <category>explainable_ai</category>
            
          </item>
        
    
        
          <item>
            <title>Head for Gaze in the wild</title>
            <description>&lt;h2 id=&quot;what-&quot;&gt;What ?&lt;/h2&gt;

&lt;p&gt;Gaze direction estimation by learning head pose information from facial images.&lt;/p&gt;

&lt;h2 id=&quot;why-&quot;&gt;Why ?&lt;/h2&gt;

&lt;p&gt;Appearance based method are considered an efficient method to learn facial information for gaze prediction. Other existing method have worked on similar problem for a controlled scenario(s) or using head pose information as brute-force to learn the mapping.&lt;/p&gt;

&lt;h2 id=&quot;how-&quot;&gt;How ?&lt;/h2&gt;

&lt;p&gt;The authors proposes two different method to learn mapping when head pose / facial image is present (HPGD), and when not available (HGD-noHP). The main intention is to find a relationship between eye deformation and head transformation.&lt;/p&gt;

&lt;p&gt;Input images are fed to a facial key-point detector which is fed to two separate module responsible for learning head poses and gaze. The hidden ‘learned’ features from both the modules are then concatenated to feed to a regressor.&lt;/p&gt;

&lt;div align=&quot;center&quot; class=&quot;img-container&quot; style=&quot;margin-top:2%&quot;&gt;

    &lt;img src=&quot;/img/head_gaze_wild/HPGD.png&quot; width=&quot;80%&quot; height=&quot;60%&quot; border=&quot;1px&quot; alt=&quot;arch&quot; class=&quot;center&quot; /&gt;
    &lt;figcaption&gt;HPGD&lt;/figcaption&gt;
&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;if facial_image.is_available():
    ## HPGD
    Two different training methods are evaluated :
      1. Implicit training : Joint Learning.
      2. Explicit training: freezing the head pose estimator network after its indivisual training and use its inference to train gaze estimator.

else:
    ## HGD-noHP
    Joint learning
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div align=&quot;center&quot; class=&quot;img-container&quot; style=&quot;margin-top:2%&quot;&gt;

    &lt;img src=&quot;/img/head_gaze_wild/HGD-noHP.png&quot; width=&quot;70%&quot; height=&quot;60%&quot; border=&quot;1px&quot; alt=&quot;arch&quot; class=&quot;center&quot; /&gt;
    &lt;figcaption&gt;HGD-noHP&lt;/figcaption&gt;
&lt;/div&gt;

&lt;p&gt;The network outputs a 3D head-pose angle (pitch, yaw).&lt;/p&gt;

&lt;h3 id=&quot;notes&quot;&gt;Notes:&lt;/h3&gt;
&lt;p&gt;The ablation study was evaluated on three main dataset (for facial image) and two for eyes image learning and the network works best for all of them. Authors also suggest using BEC (Both Eyes concatenated on Channel) method for pre-processing of eye images, instead of handling them individually (CA - Component Analysis).&lt;/p&gt;
</description>
            <pubDate>Thu, 24 Jun 2021 00:00:00 +0200</pubDate>
            <link>http://localhost:4000/blog/head-gaze-in-the-wild</link>
            <guid isPermaLink="true">http://localhost:4000/blog/head-gaze-in-the-wild</guid>
            
            <category>gaze-prediction</category>
            
            <category>pose-estimation</category>
            
            <category>egocentric</category>
            
            
            <category>gaze_prediction</category>
            
            <category>head_pose_estimation</category>
            
          </item>
        
    
        
          <item>
            <title>Integrate Gaze Attention</title>
            <description>&lt;h2 id=&quot;what-&quot;&gt;What ?&lt;/h2&gt;

&lt;p&gt;Direct optimisation and sampling technique for better integrating gaze for FPV action recognition on EGTEA dataset.&lt;/p&gt;

&lt;h2 id=&quot;why-&quot;&gt;Why ?&lt;/h2&gt;

&lt;p&gt;Improvement to the paper &lt;a href=&quot;https://sanketsans.github.io/project/in-the-eyes-of-beholder&quot;&gt;In The Eyes of Beholder&lt;/a&gt;. &lt;br /&gt;
&lt;a href=&quot;https://www.youtube.com/watch?v=JFgXEbgcT7g&quot;&gt;Gumble-Softmax&lt;/a&gt; sampling used, introduces a significant bias to a gradient estimator, limiting its performance. Also a direct integration of sampled gaze map can cause confusion due to inaccurate gaze measurements.&lt;/p&gt;

&lt;h2 id=&quot;how-&quot;&gt;How ?&lt;/h2&gt;

&lt;p&gt;The authors use the same loss function as that of their competitors. Instead of curating an attention grid, they sample the gaze points through &lt;a href=&quot;https://www.youtube.com/watch?v=JFgXEbgcT7g&quot;&gt;Gumble-Max&lt;/a&gt; trick.&lt;/p&gt;
&lt;div align=&quot;center&quot; class=&quot;img-container&quot; style=&quot;margin-top:2%&quot;&gt;

    &lt;img src=&quot;/img/integrate_gaze/gaze_sample.png&quot; width=&quot;684&quot; height=&quot;684&quot; border=&quot;1px&quot; alt=&quot;arch&quot; class=&quot;center&quot; /&gt;
    &lt;!-- &lt;figcaption&gt;DRANet architecture&lt;/figcaption&gt; --&gt;
&lt;/div&gt;
&lt;p&gt;Gamma is the random samples from a Gumble distribution.&lt;/p&gt;

&lt;p&gt;Now, they propose to formulate log-likelihood using the class-wise log-probabilities as a function of x, image features and z, gaze sampled.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;: log(pθ)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div align=&quot;center&quot; class=&quot;img-container&quot; style=&quot;margin-top:2%&quot;&gt;

    &lt;img src=&quot;/img/integrate_gaze/log_prob.png&quot; width=&quot;70%&quot; height=&quot;60%&quot; border=&quot;1px&quot; alt=&quot;arch&quot; class=&quot;center&quot; /&gt;
    &lt;!-- &lt;figcaption&gt;DRANet architecture&lt;/figcaption&gt; --&gt;
&lt;/div&gt;

&lt;p&gt;The gradient will then be a sum of multiple expectation terms of the class- wise log-probabilities, each multiplied by an indicator function, where each class gradient estimator is computed using direct optimisation.&lt;/p&gt;

&lt;div align=&quot;center&quot; class=&quot;img-container&quot; style=&quot;margin-top:2%&quot;&gt;

    &lt;img src=&quot;/img/integrate_gaze/gradient.png&quot; width=&quot;70%&quot; height=&quot;60%&quot; border=&quot;1px&quot; alt=&quot;arch&quot; class=&quot;center&quot; /&gt;
    &lt;!-- &lt;figcaption&gt;DRANet architecture&lt;/figcaption&gt; --&gt;
&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;where,
: hφ(x, z) = log qφ(z|x)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The unbiased parameter is introduced using a perturbation parameter, which is set to high initially and then progressively lowered.&lt;/p&gt;
&lt;div align=&quot;center&quot; class=&quot;img-container&quot; style=&quot;margin-top:2%&quot;&gt;

    &lt;img src=&quot;/img/integrate_gaze/unbias_param.png&quot; width=&quot;70%&quot; height=&quot;60%&quot; border=&quot;1px&quot; alt=&quot;arch&quot; class=&quot;center&quot; /&gt;
    &lt;!-- &lt;figcaption&gt;DRANet architecture&lt;/figcaption&gt; --&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot; class=&quot;img-container&quot; style=&quot;margin-top:5%&quot;&gt;

    &lt;img src=&quot;/img/integrate_gaze/arch.png&quot; width=&quot;70%&quot; height=&quot;60%&quot; border=&quot;1px&quot; alt=&quot;arch&quot; class=&quot;center&quot; /&gt;
    &lt;figcaption&gt;Architecture&lt;/figcaption&gt;
&lt;/div&gt;

&lt;p&gt;The sampled gaze distribution is fed to a FC layer followed a sigmoid to act as a soft-attention map.&lt;/p&gt;

&lt;h2 id=&quot;notes-&quot;&gt;Notes :&lt;/h2&gt;
&lt;p&gt;Structured gaze modelling with direct optimisation performs better than other reparameterization technique such as Gumble-Softmax.&lt;/p&gt;
</description>
            <pubDate>Wed, 23 Jun 2021 00:00:00 +0200</pubDate>
            <link>http://localhost:4000/blog/integrate-gaze-attention</link>
            <guid isPermaLink="true">http://localhost:4000/blog/integrate-gaze-attention</guid>
            
            <category>attention</category>
            
            <category>action-recognition</category>
            
            <category>egocentric</category>
            
            
            <category>attention</category>
            
            <category>action_recognition</category>
            
          </item>
        
    
        
          <item>
            <title>TCN for action anticipation</title>
            <description>&lt;h2 id=&quot;what-&quot;&gt;What ?&lt;/h2&gt;

&lt;p&gt;Multi Modal based action anticipation approach using temporal convolutional networks.&lt;/p&gt;

&lt;h2 id=&quot;why-&quot;&gt;Why ?&lt;/h2&gt;

&lt;p&gt;Recurrent Network (LSTM) based network are not efficient with inference and causes latency.&lt;/p&gt;

&lt;h2 id=&quot;how-&quot;&gt;How ?&lt;/h2&gt;

&lt;p&gt;Each clip is divided into ‘observed video’ and ‘action anticipation’ segment. The ‘observed’ segment is further split into smaller splits with single frame(RGB, object) and 5 frame(optical flow: horizontal and vertical component) for each snippet.
For each modality (RGB, Optical flow and Object recognition) a uni-modal network is pre-trained using a hierarchy of dilated temporal convolution nets with residual blocks.&lt;/p&gt;

&lt;div align=&quot;center&quot; class=&quot;img-container&quot; style=&quot;margin-top:2%&quot;&gt;

    &lt;img src=&quot;/img/TCN_AA/uni_arch.png&quot; width=&quot;512&quot; height=&quot;512&quot; border=&quot;1px&quot; alt=&quot;arch&quot; class=&quot;center&quot; /&gt;
    &lt;!-- &lt;figcaption&gt;DRANet architecture&lt;/figcaption&gt; --&gt;
&lt;/div&gt;
&lt;p&gt;Backbone networks :&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;RGB / Optical flow : TBN (Temporal Binding Network &lt;a href=&quot;https://arxiv.org/pdf/1908.08498.pdf&quot;&gt;Kazakos et. al ICCV’19&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;Object recognition : Fast-CNN&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Features from different modalities are fused together in a pair-wise and mutual embeddings.&lt;/p&gt;

&lt;div align=&quot;center&quot; class=&quot;img-container&quot; style=&quot;margin-top:2%&quot;&gt;

    &lt;img src=&quot;/img/TCN_AA/arch.png&quot; width=&quot;512&quot; height=&quot;256&quot; border=&quot;1px&quot; alt=&quot;arch&quot; class=&quot;center&quot; /&gt;
    &lt;!-- &lt;figcaption&gt;DRANet architecture&lt;/figcaption&gt; --&gt;
&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import numpy as np
rgb = np.random.randn(1024)
flow = np.random.randn(1024)
obj = np.random.rand(1024)

pair_rgb_obj = np.random.randn(rgb + obj)
pair_rgb_flow = np.random.randn(rgb + flow)
pair_obj_flow = np.random.randn(flow + obj)
pair = np.random.randn(pair_rgb_obj + pair_rgb_flow + pair_obj_flow)

mutual = np.random.randn(rgb + flow + obj)

final = mutual + pair
output = softmax(final)

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;notes&quot;&gt;Notes&lt;/h2&gt;
&lt;p&gt;Does not perform better than SOTA action anticipation methods. But has faster training and inference time &lt;i&gt;wrt&lt;/i&gt; RNN based network.&lt;/p&gt;
</description>
            <pubDate>Tue, 22 Jun 2021 00:00:00 +0200</pubDate>
            <link>http://localhost:4000/blog/tcn-action-anticipation</link>
            <guid isPermaLink="true">http://localhost:4000/blog/tcn-action-anticipation</guid>
            
            <category>multi-modal</category>
            
            <category>action-anticipation</category>
            
            <category>egocentric</category>
            
            
            <category>multi_modal</category>
            
            <category>action_anticipation</category>
            
            <category>fusion</category>
            
          </item>
        
    
        
          <item>
            <title>In The Eyes of Beholder</title>
            <description>&lt;h2 id=&quot;what-&quot;&gt;What ?&lt;/h2&gt;

&lt;p&gt;Joint gaze estimation and action recognition using gaze distribution as attention. New dataset EGTEA (fine-grained actions) - 28hrs with 2D gaze labelling.&lt;/p&gt;

&lt;h2 id=&quot;why-&quot;&gt;Why ?&lt;/h2&gt;

&lt;p&gt;Gaze is used for FPV action recognition since it naturally defined the ROI which can be used for attention based network. Gaze measurements are also quite in-consistent and causes uncertainty in modelling deep networks which is why instead of 2D gaze coordinates, a probabilistic distribution is preferred for modelling the deep network.&lt;/p&gt;

&lt;h2 id=&quot;how-&quot;&gt;How ?&lt;/h2&gt;

&lt;p&gt;The authors use 2D gaze coordinates and transform it to a probabilistic distribution of gaze to create a saliency map for the image plane.&lt;/p&gt;

&lt;p&gt;The saliency acts as an attention grid for the input frame(s), which is used to extract visual features from the image. - weighted average pooling. The extracted features are then fed to a classifier which predicts the action.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;: empty_plane = np.zeros(m,n)
: empty_plane[x][y] = 1 ## gaze position
: q(m,n) = multivariate_normal(q(m,n))  ## probabilistic distribution , when no gaze is available, instead use a uniform distribution.
: g'(m,n) &amp;lt;- q(m,n) ## gaze attention map
: p(g|x) = g'(m,n)

: π = qψ(X) ## output from the neural network, distribution achieved by the network
: q(g|x) = π(m,n)

: p(y|g, x) = f (Σ(m,n) π(m,n) φ(x)(m,n) ) = SOFTMAX(W_t (Σ(m,n) π(m,n) φ(x)(m,n))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div align=&quot;center&quot; class=&quot;img-container&quot; style=&quot;margin-top:2%&quot;&gt;

    &lt;img src=&quot;/img/In_The_Eyes/arch.png&quot; width=&quot;70%&quot; height=&quot;60%&quot; border=&quot;1px&quot; alt=&quot;arch&quot; class=&quot;center&quot; /&gt;
    &lt;!-- &lt;figcaption&gt;DRANet architecture&lt;/figcaption&gt; --&gt;
&lt;/div&gt;

&lt;p&gt;The sampling from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;π(m,n)&lt;/code&gt; is done using Gumbel-Softmax approach to make the entire pipeline fully differentiable instead of direct sampling.&lt;/p&gt;

&lt;div align=&quot;center&quot; class=&quot;img-container&quot; style=&quot;margin-top:2%&quot;&gt;
    &lt;img src=&quot;/img/In_The_Eyes/direct_samp.png&quot; width=&quot;70%&quot; height=&quot;50%&quot; border=&quot;1px&quot; alt=&quot;arch&quot; class=&quot;center&quot; /&gt;
    &lt;figcaption&gt;Direct sampling&lt;/figcaption&gt;

    &lt;img src=&quot;/img/In_The_Eyes/gumble_samp.png&quot; width=&quot;70%&quot; height=&quot;50%&quot; border=&quot;1px&quot; alt=&quot;arch&quot; class=&quot;center&quot; style=&quot;margin-top:2%&quot; /&gt;
    &lt;figcaption&gt;Gumbel sampling&lt;/figcaption&gt;
&lt;/div&gt;

&lt;p&gt;Since, the network has two main goals to achieve : Create accurate gaze distribution and predict correct action. Therefore, the loss function is the negative of empirical lower bound :&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Loss : −Σ log(p(y|g,x)) + KL[q(g|x)||p(g|x)]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;TL;DR&lt;/em&gt;&lt;/strong&gt; Actions and Hand Mask Annotations.&lt;/p&gt;

&lt;h3 id=&quot;notes-&quot;&gt;Notes :&lt;/h3&gt;

&lt;p&gt;The action classification accuracy is best when combined with gaze probability / attention on EGTEA dataset. When trained and tested on Epic-Kitchen, it performs on par with SOTA but not better, given that EPIC-Kitchen does not have gaze labelling.&lt;/p&gt;
</description>
            <pubDate>Mon, 21 Jun 2021 00:00:00 +0200</pubDate>
            <link>http://localhost:4000/blog/in-the-eyes-of-beholder</link>
            <guid isPermaLink="true">http://localhost:4000/blog/in-the-eyes-of-beholder</guid>
            
            <category>attention</category>
            
            <category>action-recognition</category>
            
            <category>gaze-prediction</category>
            
            <category>egocentric</category>
            
            
            <category>attention</category>
            
            <category>action_recognition</category>
            
            <category>gaze_prediction</category>
            
          </item>
        
    
        
          <item>
            <title>DRANet</title>
            <description>&lt;h2 id=&quot;what-&quot;&gt;What ?&lt;/h2&gt;

&lt;p&gt;Person Re-ID using a dual reverse attention based module. Creates hard examples by transforming features using channel and spatial reverse attention process.&lt;/p&gt;

&lt;h2 id=&quot;why-&quot;&gt;Why ?&lt;/h2&gt;

&lt;p&gt;Inability to generalise on hard examples(occlusion, light variances etc.). Mining &lt;i&gt;/&lt;/i&gt; synthetic data generation is expensive and suffers on convergence problem.&lt;/p&gt;

&lt;h2 id=&quot;how-&quot;&gt;How ?&lt;/h2&gt;

&lt;p&gt;The authors introduce &lt;b&gt;DRANet&lt;/b&gt; (Dual Reverse Attention Networks) to generate hard examples in convolutional feature space.&lt;/p&gt;

&lt;div align=&quot;center&quot; class=&quot;img-container&quot;&gt;

    &lt;img src=&quot;/img/dranet/arch.png&quot; width=&quot;70%&quot; height=&quot;60%&quot; border=&quot;1px&quot; alt=&quot;arch&quot; class=&quot;center&quot; /&gt;
    &lt;!-- &lt;figcaption&gt;DRANet architecture&lt;/figcaption&gt; --&gt;
&lt;/div&gt;

&lt;p&gt;The input image is fed to ResNet50 and features maps are divided into three parts (&lt;b&gt;Why ?&lt;/b&gt;). Each partial feature sequentially goes through CIC (Channel Information Capture) and SIC (Spatial Information Capture).&lt;/p&gt;

&lt;h4 id=&quot;dual-reverse-attention&quot;&gt;Dual Reverse Attention&lt;/h4&gt;

&lt;div align=&quot;center&quot; class=&quot;img-container&quot;&gt;
    &lt;img src=&quot;/img/dranet/rcam.png&quot; alt=&quot;rcam&quot; width=&quot;60%&quot; height=&quot;45%&quot; /&gt;
    &lt;figcaption&gt;Channel based reverse Attention&lt;/figcaption&gt;

    &lt;img src=&quot;/img/dranet/rsam.png&quot; alt=&quot;rsam&quot; width=&quot;60%&quot; height=&quot;45%&quot; /&gt;
    &lt;figcaption&gt;Spatial based reverse Attention&lt;/figcaption&gt;
&lt;/div&gt;

&lt;p&gt;CIC provides a mask ‘m’ to suppress informative features and generate channel based hard examples.
&lt;!-- The authors use a squeeze function (FCL) to extract descriptive features for each channel and an excitation function to reflect relative importance of each channel (m) . Later they are combined using channel-wise multiplication for informative features.  &lt;b&gt; (B) &lt;/b&gt; --&gt;
Similarly, SIC provides a mask, but &lt;b&gt;‘only for’&lt;/b&gt; corresponding person identity &lt;i&gt;‘n’&lt;/i&gt; (Only available during training).&lt;/p&gt;

&lt;h3 id=&quot;implementation&quot;&gt;Implementation&lt;/h3&gt;

&lt;p&gt;All &lt;b&gt;three&lt;/b&gt; softmax losses are combined during training process. During testing, only B and SIC is possible (since person identity is available during training only for spatial reverse attention). Encodings from both features maps after GAP are concatenated (2048 - 1024*2) for predictions.&lt;/p&gt;

&lt;h3 id=&quot;notes-&quot;&gt;Notes :&lt;/h3&gt;

&lt;p&gt;The model improves mAP on three dataset Market-150, DukeMTMC-reID, CUHK03 by 2, 3 &amp;amp; 5 %. But performs almost similar for rank 1, 5, and 10 accuracy.
For details check out the original paper &lt;a href=&quot;https://ieeexplore.ieee.org/document/8804419&quot;&gt;DRANET&lt;/a&gt;.&lt;/p&gt;
</description>
            <pubDate>Tue, 15 Jun 2021 00:00:00 +0200</pubDate>
            <link>http://localhost:4000/blog/dranet</link>
            <guid isPermaLink="true">http://localhost:4000/blog/dranet</guid>
            
            <category>attention</category>
            
            
            <category>attention</category>
            
          </item>
        
    
  </channel>
</rss>
