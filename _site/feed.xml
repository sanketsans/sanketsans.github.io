<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title></title>
    <description></description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Fri, 25 Jun 2021 17:15:54 +0200</pubDate>
    <lastBuildDate>Fri, 25 Jun 2021 17:15:54 +0200</lastBuildDate>
    <generator>Jekyll v3.8.4</generator>
    
        
          <item>
            <title>FACExplanations</title>
            <description>&lt;h2 id=&quot;what-&quot;&gt;What ?&lt;/h2&gt;

&lt;p&gt;Feasible and Actionable counterfactuals using high density paths of data points and custom weight functions.&lt;/p&gt;

&lt;h2 id=&quot;why-&quot;&gt;Why ?&lt;/h2&gt;

&lt;p&gt;Counterfactuals does not consider underlying data distributions and constraints that makes them unachievable and infeasible.&lt;/p&gt;

&lt;h2 id=&quot;how-&quot;&gt;How ?&lt;/h2&gt;

&lt;p&gt;A graph over the data points is constructed where edges are weighted using different metrics based on distances and density of data points. This metric can be tuned based on properties of target instance (to be explained counterfactually) - prediction threshold (for distance) and density threshold.&lt;/p&gt;

&lt;div align=&quot;center&quot; class=&quot;img-container&quot; style=&quot;margin-top:2%&quot;&gt;

    &lt;img src=&quot;/img/FACE/counter_path.png&quot; width=&quot;80%&quot; height=&quot;60%&quot; border=&quot;1px&quot; alt=&quot;arch&quot; class=&quot;center&quot; /&gt;
    &lt;figcaption&gt;HPGD&lt;/figcaption&gt;
&lt;/div&gt;

&lt;p&gt;For the given data points graph, FACE suggests D to be the best counterfactual. As A has low classification margin and is in low-density region. B has good classification margin but again is in low-density. C is in high density but is not connected by a high density path.&lt;/p&gt;

&lt;div align=&quot;center&quot; class=&quot;img-container&quot; style=&quot;margin-top:2%&quot;&gt;

    &lt;img src=&quot;/img/FACE/epsilon_low.png&quot; width=&quot;80%&quot; height=&quot;60%&quot; border=&quot;1px&quot; alt=&quot;arch&quot; class=&quot;center&quot; /&gt;
    &lt;figcaption&gt;Low threshold&lt;/figcaption&gt;
&lt;/div&gt;

&lt;p&gt;The counterfactuals generated ensures a path that guarantees feasible changes (due to high density data points). Furthermore, a custom weight and condition function is provided to remove inappropriate edges. The figure compares the differences in constructing counterfactuals by typical KDE (Kernel Density Estimator) approach (Left) and FACE approach (Right).&lt;/p&gt;

&lt;div align=&quot;center&quot; class=&quot;img-container&quot; style=&quot;margin-top:2%&quot;&gt;

    &lt;img src=&quot;/img/FACE/epsilon_high.png&quot; width=&quot;80%&quot; height=&quot;60%&quot; border=&quot;1px&quot; alt=&quot;arch&quot; class=&quot;center&quot; /&gt;
    &lt;figcaption&gt;High threshold&lt;/figcaption&gt;
&lt;/div&gt;

&lt;p&gt;However, the paths can also be decided based on a distance threshold metric, &lt;i&gt;ε&lt;/i&gt; , when the user wants to somewhat ignore the density metric. Different use cases can use different density thresholds based on the distribution of the data.&lt;/p&gt;

&lt;h3 id=&quot;notes&quot;&gt;Notes&lt;/h3&gt;
&lt;p&gt;Assumes covariates to be distributed only on the basis of density. Actionable counterfactuals could be generated based on input constraints as well.&lt;/p&gt;
</description>
            <pubDate>Fri, 25 Jun 2021 00:00:00 +0200</pubDate>
            <link>http://localhost:4000/blog/facexplanations</link>
            <guid isPermaLink="true">http://localhost:4000/blog/facexplanations</guid>
            
            <category>explainable_ai</category>
            
            <category>counterfactuals</category>
            
            
            <category>explainable_ai</category>
            
            <category>counterfactuals</category>
            
          </item>
        
    
        
          <item>
            <title>Head for Gaze in the wild</title>
            <description>&lt;h2 id=&quot;what-&quot;&gt;What ?&lt;/h2&gt;

&lt;p&gt;Gaze direction estimation by learning head pose information from facial images.&lt;/p&gt;

&lt;h2 id=&quot;why-&quot;&gt;Why ?&lt;/h2&gt;

&lt;p&gt;Appearance based method are considered an efficient method to learn facial information for gaze prediction. Other existing method have worked on similar problem for a controlled scenario(s) or using head pose information as brute-force to learn the mapping.&lt;/p&gt;

&lt;h2 id=&quot;how-&quot;&gt;How ?&lt;/h2&gt;

&lt;p&gt;The authors proposes two different method to learn mapping when head pose / facial image is present (HPGD), and when not available (HGD-noHP). The main intention is to find a relationship between eye deformation and head transformation.&lt;/p&gt;

&lt;p&gt;Input images are fed to a facial key-point detector which is fed to two separate module responsible for learning head poses and gaze. The hidden ‘learned’ features from both the modules are then concatenated to feed to a regressor.&lt;/p&gt;

&lt;div align=&quot;center&quot; class=&quot;img-container&quot; style=&quot;margin-top:2%&quot;&gt;

    &lt;img src=&quot;/img/head_gaze_wild/HPGD.png&quot; width=&quot;80%&quot; height=&quot;60%&quot; border=&quot;1px&quot; alt=&quot;arch&quot; class=&quot;center&quot; /&gt;
    &lt;figcaption&gt;HPGD&lt;/figcaption&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;if facial_image.is_available():
    ## HPGD
    Two different training methods are evaluated :
      1. Implicit training : Joint Learning.
      2. Explicit training: freezing the head pose estimator network after its indivisual training and use its inference to train gaze estimator.

else:
    ## HGD-noHP
    Joint learning
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div align=&quot;center&quot; class=&quot;img-container&quot; style=&quot;margin-top:2%&quot;&gt;

    &lt;img src=&quot;/img/head_gaze_wild/HGD-noHP.png&quot; width=&quot;70%&quot; height=&quot;60%&quot; border=&quot;1px&quot; alt=&quot;arch&quot; class=&quot;center&quot; /&gt;
    &lt;figcaption&gt;HGD-noHP&lt;/figcaption&gt;
&lt;/div&gt;

&lt;p&gt;The network outputs a 3D head-pose angle (pitch, yaw).&lt;/p&gt;

&lt;h3 id=&quot;notes&quot;&gt;Notes:&lt;/h3&gt;
&lt;p&gt;The ablation study was evaluated on three main dataset (for facial image) and two for eyes image learning and the network works best for all of them. Authors also suggest using BEC (Both Eyes concatenated on Channel) method for pre-processing of eye images, instead of handling them individually (CA - Component Analysis).&lt;/p&gt;
</description>
            <pubDate>Thu, 24 Jun 2021 00:00:00 +0200</pubDate>
            <link>http://localhost:4000/blog/head-gaze-in-the-wild</link>
            <guid isPermaLink="true">http://localhost:4000/blog/head-gaze-in-the-wild</guid>
            
            <category>gaze_prediction</category>
            
            <category>head_pose_estimation</category>
            
            
            <category>gaze_prediction</category>
            
            <category>head_pose_estimation</category>
            
          </item>
        
    
        
          <item>
            <title>Integrate Gaze Attention</title>
            <description>&lt;h2 id=&quot;what-&quot;&gt;What ?&lt;/h2&gt;

&lt;p&gt;Direct optimisation and sampling technique for better integrating gaze for FPV action recognition on EGTEA dataset.&lt;/p&gt;

&lt;h2 id=&quot;why-&quot;&gt;Why ?&lt;/h2&gt;

&lt;p&gt;Improvement to the paper &lt;a href=&quot;https://sanketsans.github.io/project/in-the-eyes-of-beholder&quot;&gt;In The Eyes of Beholder&lt;/a&gt;. &lt;br /&gt;
&lt;a href=&quot;https://www.youtube.com/watch?v=JFgXEbgcT7g&quot;&gt;Gumble-Softmax&lt;/a&gt; sampling used, introduces a significant bias to a gradient estimator, limiting its performance. Also a direct integration of sampled gaze map can cause confusion due to inaccurate gaze measurements.&lt;/p&gt;

&lt;h2 id=&quot;how-&quot;&gt;How ?&lt;/h2&gt;

&lt;p&gt;The authors use the same loss function as that of their competitors. Instead of curating an attention grid, they sample the gaze points through &lt;a href=&quot;https://www.youtube.com/watch?v=JFgXEbgcT7g&quot;&gt;Gumble-Max&lt;/a&gt; trick.&lt;/p&gt;
&lt;div align=&quot;center&quot; class=&quot;img-container&quot; style=&quot;margin-top:2%&quot;&gt;

    &lt;img src=&quot;/img/integrate_gaze/gaze_sample.png&quot; width=&quot;684&quot; height=&quot;684&quot; border=&quot;1px&quot; alt=&quot;arch&quot; class=&quot;center&quot; /&gt;
    &lt;!-- &lt;figcaption&gt;DRANet architecture&lt;/figcaption&gt; --&gt;
&lt;/div&gt;
&lt;p&gt;Gamma is the random samples from a Gumble distribution.&lt;/p&gt;

&lt;p&gt;Now, they propose to formulate log-likelihood using the class-wise log-probabilities as a function of x, image features and z, gaze sampled.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;: log(pθ)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div align=&quot;center&quot; class=&quot;img-container&quot; style=&quot;margin-top:2%&quot;&gt;

    &lt;img src=&quot;/img/integrate_gaze/log_prob.png&quot; width=&quot;70%&quot; height=&quot;60%&quot; border=&quot;1px&quot; alt=&quot;arch&quot; class=&quot;center&quot; /&gt;
    &lt;!-- &lt;figcaption&gt;DRANet architecture&lt;/figcaption&gt; --&gt;
&lt;/div&gt;

&lt;p&gt;The gradient will then be a sum of multiple expectation terms of the class- wise log-probabilities, each multiplied by an indicator function, where each class gradient estimator is computed using direct optimisation.&lt;/p&gt;

&lt;div align=&quot;center&quot; class=&quot;img-container&quot; style=&quot;margin-top:2%&quot;&gt;

    &lt;img src=&quot;/img/integrate_gaze/gradient.png&quot; width=&quot;70%&quot; height=&quot;60%&quot; border=&quot;1px&quot; alt=&quot;arch&quot; class=&quot;center&quot; /&gt;
    &lt;!-- &lt;figcaption&gt;DRANet architecture&lt;/figcaption&gt; --&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;where,
: hφ(x, z) = log qφ(z|x)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The unbiased parameter is introduced using a perturbation parameter, which is set to high initially and then progressively lowered.&lt;/p&gt;
&lt;div align=&quot;center&quot; class=&quot;img-container&quot; style=&quot;margin-top:2%&quot;&gt;

    &lt;img src=&quot;/img/integrate_gaze/unbias_param.png&quot; width=&quot;70%&quot; height=&quot;60%&quot; border=&quot;1px&quot; alt=&quot;arch&quot; class=&quot;center&quot; /&gt;
    &lt;!-- &lt;figcaption&gt;DRANet architecture&lt;/figcaption&gt; --&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot; class=&quot;img-container&quot; style=&quot;margin-top:5%&quot;&gt;

    &lt;img src=&quot;/img/integrate_gaze/arch.png&quot; width=&quot;70%&quot; height=&quot;60%&quot; border=&quot;1px&quot; alt=&quot;arch&quot; class=&quot;center&quot; /&gt;
    &lt;figcaption&gt;Architecture&lt;/figcaption&gt;
&lt;/div&gt;

&lt;p&gt;The sampled gaze distribution is fed to a FC layer followed a sigmoid to act as a soft-attention map.&lt;/p&gt;

&lt;h2 id=&quot;notes-&quot;&gt;Notes :&lt;/h2&gt;
&lt;p&gt;Structured gaze modelling with direct optimisation performs better than other reparameterization technique such as Gumble-Softmax.&lt;/p&gt;
</description>
            <pubDate>Wed, 23 Jun 2021 00:00:00 +0200</pubDate>
            <link>http://localhost:4000/blog/integrate-gaze-attention</link>
            <guid isPermaLink="true">http://localhost:4000/blog/integrate-gaze-attention</guid>
            
            <category>attention</category>
            
            <category>action_recognition</category>
            
            <category>sampling</category>
            
            
            <category>attention</category>
            
            <category>action_recognition</category>
            
            <category>sampling</category>
            
          </item>
        
    
        
          <item>
            <title>TCN for action anticipation</title>
            <description>&lt;h2 id=&quot;what-&quot;&gt;What ?&lt;/h2&gt;

&lt;p&gt;Multi Modal based action anticipation approach using temporal convolutional networks.&lt;/p&gt;

&lt;h2 id=&quot;why-&quot;&gt;Why ?&lt;/h2&gt;

&lt;p&gt;Recurrent Network (LSTM) based network are not efficient with inference and causes latency.&lt;/p&gt;

&lt;h2 id=&quot;how-&quot;&gt;How ?&lt;/h2&gt;

&lt;p&gt;Each clip is divided into ‘observed video’ and ‘action anticipation’ segment. The ‘observed’ segment is further split into smaller splits with single frame(RGB, object) and 5 frame(optical flow: horizontal and vertical component) for each snippet.
For each modality (RGB, Optical flow and Object recognition) a uni-modal network is pre-trained using a hierarchy of dilated temporal convolution nets with residual blocks.&lt;/p&gt;

&lt;div align=&quot;center&quot; class=&quot;img-container&quot; style=&quot;margin-top:2%&quot;&gt;

    &lt;img src=&quot;/img/TCN_AA/uni_arch.png&quot; width=&quot;512&quot; height=&quot;512&quot; border=&quot;1px&quot; alt=&quot;arch&quot; class=&quot;center&quot; /&gt;
    &lt;!-- &lt;figcaption&gt;DRANet architecture&lt;/figcaption&gt; --&gt;
&lt;/div&gt;
&lt;p&gt;Backbone networks :&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;RGB / Optical flow : TBN (Temporal Binding Network &lt;a href=&quot;https://arxiv.org/pdf/1908.08498.pdf&quot;&gt;Kazakos et. al ICCV’19&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;Object recognition : Fast-CNN&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Features from different modalities are fused together in a pair-wise and mutual embeddings.&lt;/p&gt;

&lt;div align=&quot;center&quot; class=&quot;img-container&quot; style=&quot;margin-top:2%&quot;&gt;

    &lt;img src=&quot;/img/TCN_AA/arch.png&quot; width=&quot;512&quot; height=&quot;256&quot; border=&quot;1px&quot; alt=&quot;arch&quot; class=&quot;center&quot; /&gt;
    &lt;!-- &lt;figcaption&gt;DRANet architecture&lt;/figcaption&gt; --&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import numpy as np
rgb = np.random.randn(1024)
flow = np.random.randn(1024)
obj = np.random.rand(1024)

pair_rgb_obj = np.random.randn(rgb + obj)
pair_rgb_flow = np.random.randn(rgb + flow)
pair_obj_flow = np.random.randn(flow + obj)
pair = np.random.randn(pair_rgb_obj + pair_rgb_flow + pair_obj_flow)

mutual = np.random.randn(rgb + flow + obj)

final = mutual + pair
output = softmax(final)

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;notes&quot;&gt;Notes&lt;/h2&gt;
&lt;p&gt;Does not perform better than SOTA action anticipation methods. But has faster training and inference time &lt;i&gt;wrt&lt;/i&gt; RNN based network.&lt;/p&gt;
</description>
            <pubDate>Tue, 22 Jun 2021 00:00:00 +0200</pubDate>
            <link>http://localhost:4000/blog/tcn-action-anticipation</link>
            <guid isPermaLink="true">http://localhost:4000/blog/tcn-action-anticipation</guid>
            
            <category>multi_modal</category>
            
            <category>action_anticipation</category>
            
            <category>fusion</category>
            
            
            <category>multi_modal</category>
            
            <category>action_anticipation</category>
            
            <category>fusion</category>
            
          </item>
        
    
        
          <item>
            <title>In The Eyes of Beholder</title>
            <description>&lt;h2 id=&quot;what-&quot;&gt;What ?&lt;/h2&gt;

&lt;p&gt;Joint gaze estimation and action recognition using gaze distribution as attention. New dataset EGTEA (fine-grained actions) - 28hrs with 2D gaze labelling.&lt;/p&gt;

&lt;h2 id=&quot;why-&quot;&gt;Why ?&lt;/h2&gt;

&lt;p&gt;Gaze is used for FPV action recognition since it naturally defined the ROI which can be used for attention based network. Gaze measurements are also quite in-consistent and causes uncertainty in modelling deep networks which is why instead of 2D gaze coordinates, a probabilistic distribution is preferred for modelling the deep network.&lt;/p&gt;

&lt;h2 id=&quot;how-&quot;&gt;How ?&lt;/h2&gt;

&lt;p&gt;The authors use 2D gaze coordinates and transform it to a probabilistic distribution of gaze to create a saliency map for the image plane.&lt;/p&gt;

&lt;p&gt;The saliency acts as an attention grid for the input frame(s), which is used to extract visual features from the image. - weighted average pooling. The extracted features are then fed to a classifier which predicts the action.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;: empty_plane = np.zeros(m,n)
: empty_plane[x][y] = 1 ## gaze position
: q(m,n) = multivariate_normal(q(m,n))  ## probabilistic distribution , when no gaze is available, instead use a uniform distribution.
: g'(m,n) &amp;lt;- q(m,n) ## gaze attention map
: p(g|x) = g'(m,n)

: π = qψ(X) ## output from the neural network, distribution achieved by the network
: q(g|x) = π(m,n)

: p(y|g, x) = f (Σ(m,n) π(m,n) φ(x)(m,n) ) = SOFTMAX(W_t (Σ(m,n) π(m,n) φ(x)(m,n))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div align=&quot;center&quot; class=&quot;img-container&quot; style=&quot;margin-top:2%&quot;&gt;

    &lt;img src=&quot;/img/In_The_Eyes/arch.png&quot; width=&quot;70%&quot; height=&quot;60%&quot; border=&quot;1px&quot; alt=&quot;arch&quot; class=&quot;center&quot; /&gt;
    &lt;!-- &lt;figcaption&gt;DRANet architecture&lt;/figcaption&gt; --&gt;
&lt;/div&gt;

&lt;p&gt;The sampling from &lt;code class=&quot;highlighter-rouge&quot;&gt;π(m,n)&lt;/code&gt; is done using Gumbel-Softmax approach to make the entire pipeline fully differentiable instead of direct sampling.&lt;/p&gt;

&lt;div align=&quot;center&quot; class=&quot;img-container&quot; style=&quot;margin-top:2%&quot;&gt;
    &lt;img src=&quot;/img/In_The_Eyes/direct_samp.png&quot; width=&quot;70%&quot; height=&quot;50%&quot; border=&quot;1px&quot; alt=&quot;arch&quot; class=&quot;center&quot; /&gt;
    &lt;figcaption&gt;Direct sampling&lt;/figcaption&gt;

    &lt;img src=&quot;/img/In_The_Eyes/gumble_samp.png&quot; width=&quot;70%&quot; height=&quot;50%&quot; border=&quot;1px&quot; alt=&quot;arch&quot; class=&quot;center&quot; style=&quot;margin-top:2%&quot; /&gt;
    &lt;figcaption&gt;Gumbel sampling&lt;/figcaption&gt;
&lt;/div&gt;

&lt;p&gt;Since, the network has two main goals to achieve : Create accurate gaze distribution and predict correct action. Therefore, the loss function is the negative of empirical lower bound :&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Loss : −Σ log(p(y|g,x)) + KL[q(g|x)||p(g|x)]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;TL;DR&lt;/em&gt;&lt;/strong&gt; Actions and Hand Mask Annotations.&lt;/p&gt;

&lt;h3 id=&quot;notes-&quot;&gt;Notes :&lt;/h3&gt;

&lt;p&gt;The action classification accuracy is best when combined with gaze probability / attention on EGTEA dataset. When trained and tested on Epic-Kitchen, it performs on par with SOTA but not better, given that EPIC-Kitchen does not have gaze labelling.&lt;/p&gt;
</description>
            <pubDate>Mon, 21 Jun 2021 00:00:00 +0200</pubDate>
            <link>http://localhost:4000/blog/in-the-eyes-of-beholder</link>
            <guid isPermaLink="true">http://localhost:4000/blog/in-the-eyes-of-beholder</guid>
            
            <category>attention</category>
            
            <category>action_recognition</category>
            
            <category>gaze_prediction</category>
            
            
            <category>attention</category>
            
            <category>action_recognition</category>
            
            <category>gaze_prediction</category>
            
          </item>
        
    
        
          <item>
            <title>DRANet</title>
            <description>&lt;h2 id=&quot;what-&quot;&gt;What ?&lt;/h2&gt;

&lt;p&gt;Person Re-ID using a dual reverse attention based module. Creates hard examples by transforming features using channel and spatial reverse attention process.&lt;/p&gt;

&lt;h2 id=&quot;why-&quot;&gt;Why ?&lt;/h2&gt;

&lt;p&gt;Inability to generalise on hard examples(occlusion, light variances etc.). Mining &lt;i&gt;/&lt;/i&gt; synthetic data generation is expensive and suffers on convergence problem.&lt;/p&gt;

&lt;h2 id=&quot;how-&quot;&gt;How ?&lt;/h2&gt;

&lt;p&gt;The authors introduce &lt;b&gt;DRANet&lt;/b&gt; (Dual Reverse Attention Networks) to generate hard examples in convolutional feature space.&lt;/p&gt;

&lt;div align=&quot;center&quot; class=&quot;img-container&quot;&gt;

    &lt;img src=&quot;/img/dranet/arch.png&quot; width=&quot;70%&quot; height=&quot;60%&quot; border=&quot;1px&quot; alt=&quot;arch&quot; class=&quot;center&quot; /&gt;
    &lt;!-- &lt;figcaption&gt;DRANet architecture&lt;/figcaption&gt; --&gt;
&lt;/div&gt;

&lt;p&gt;The input image is fed to ResNet50 and features maps are divided into three parts (&lt;b&gt;Why ?&lt;/b&gt;). Each partial feature sequentially goes through CIC (Channel Information Capture) and SIC (Spatial Information Capture).&lt;/p&gt;

&lt;h4 id=&quot;dual-reverse-attention&quot;&gt;Dual Reverse Attention&lt;/h4&gt;

&lt;div align=&quot;center&quot; class=&quot;img-container&quot;&gt;
    &lt;img src=&quot;/img/dranet/rcam.png&quot; alt=&quot;rcam&quot; width=&quot;60%&quot; height=&quot;45%&quot; /&gt;
    &lt;figcaption&gt;Channel based reverse Attention&lt;/figcaption&gt;

    &lt;img src=&quot;/img/dranet/rsam.png&quot; alt=&quot;rsam&quot; width=&quot;60%&quot; height=&quot;45%&quot; /&gt;
    &lt;figcaption&gt;Spatial based reverse Attention&lt;/figcaption&gt;
&lt;/div&gt;

&lt;p&gt;CIC provides a mask ‘m’ to suppress informative features and generate channel based hard examples.
&lt;!-- The authors use a squeeze function (FCL) to extract descriptive features for each channel and an excitation function to reflect relative importance of each channel (m) . Later they are combined using channel-wise multiplication for informative features.  &lt;b&gt; (B) &lt;/b&gt; --&gt;
Similarly, SIC provides a mask, but &lt;b&gt;‘only for’&lt;/b&gt; corresponding person identity &lt;i&gt;‘n’&lt;/i&gt; (Only available during training).&lt;/p&gt;

&lt;h3 id=&quot;implementation&quot;&gt;Implementation&lt;/h3&gt;

&lt;p&gt;All &lt;b&gt;three&lt;/b&gt; softmax losses are combined during training process. During testing, only B and SIC is possible (since person identity is available during training only for spatial reverse attention). Encodings from both features maps after GAP are concatenated (2048 - 1024*2) for predictions.&lt;/p&gt;

&lt;h3 id=&quot;notes-&quot;&gt;Notes :&lt;/h3&gt;

&lt;p&gt;The model improves mAP on three dataset Market-150, DukeMTMC-reID, CUHK03 by 2, 3 &amp;amp; 5 %. But performs almost similar for rank 1, 5, and 10 accuracy.
For details check out the original paper &lt;a href=&quot;https://ieeexplore.ieee.org/document/8804419&quot;&gt;DRANET&lt;/a&gt;.&lt;/p&gt;
</description>
            <pubDate>Tue, 15 Jun 2021 00:00:00 +0200</pubDate>
            <link>http://localhost:4000/blog/dranet</link>
            <guid isPermaLink="true">http://localhost:4000/blog/dranet</guid>
            
            <category>reverse_attention</category>
            
            
            <category>reverse_attention</category>
            
          </item>
        
    
  </channel>
</rss>
