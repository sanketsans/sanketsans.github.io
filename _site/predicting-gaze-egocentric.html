<script src="http://www.google.com/jsapi" type="text/javascript"></script> 
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif; 
		font-weight:300;
		font-size:18px;
		margin-left: auto;
		margin-right: auto;
		width: 1100px;
	}
	
	h1 {
		font-size:32px;
		font-weight:300;
	}
	
	.disclaimerbox {
		background-color: #eee;		
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.rounded {
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}
	
	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}
	
	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		15px 15px 0 0px #fff, /* The fourth layer */
		15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		20px 20px 0 0px #fff, /* The fifth layer */
		20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		25px 25px 0 0px #fff, /* The fifth layer */
		25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}

	.paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35); /* The top layer shadow */

		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}

	.green-text{
		color: green;
	}
	.red-text{
		color: red;
	}
	
	.vert-cent {
		position: relative;
		top: 50%;
		transform: translateY(-50%);
	}
	
	hr
	{
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}

    .swipe-container {
      width: 660px;
      height: 400px;
      background-color: #ccc;
      overflow: hidden;
	  justify-content: center;
    }
    
    .swipe-content {
      width: 1300px; /* Width of the entire content */
      height: 100%;
      display: flex;
      transition: transform 0.3s ease-out;
    }
    
    .swipe-item {
      flex: 0 0 660px; /* Width of each individual item */
      height: 100%;
      background-color: #f0f0f0;
    }

	.swipe-buttons {
		display: flex;
		justify-content: space-between;
		margin-top: 10px;
    }

    .swipe-btn {
      width: 300px;
      height: 300px;
      background-color: #143d7a;
      color: #363434;
      text-align: center;
      line-height: 30px;
      cursor: pointer;
    }
    

</style>

<html>
<head>
	<title>Predicting Gaze from Egocentric Social Interaction Videos and IMU Data</title>
	<meta property="og:image" content="Path to my teaser.png"/> <!-- Facebook automatically scrapes this. Go to https://developers.facebook.com/tools/debug/ if you update and want to force Facebook to rescrape. -->
	<meta property="og:title" content="Predicting Gaze from Egocentric Social Interaction Videos and IMU Data" />
	<meta property="og:description" content="This is the paper desc" />

	<!-- Get from Google Analytics -->
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src=""></script> 
	<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());

		gtag('config', 'UA-75863369-6');
	</script>
</head>

<body>
	<br>
	<center>
		<span style="font-size:34px">Predicting Gaze from Egocentric Social Interaction Videos and IMU Data</span><br>
		<br>
		<table align=center width=600px>
			<table align=center width=800px>
				<tr>
					<td align=center width=150px>
						<center>
							<span style="font-size:24px"><a href="https://sanketsans.github.io/about">Sanket Thakur</a></span>
						</center>
					</td>
					<td align=center width=150px>
						<center>
							<span style="font-size:24px"><a href="https://cbeyan.github.io/">Cigdem Beyan</a></span>
						</center>
					</td>
					<td align=center width=150px>
						<center>
							<span style="font-size:24px"><a href="https://scholar.google.it/citations?user=lPV9rbkAAAAJ&hl=it">Pietro Morerio</a></span>
						</center>
					</td>
					<td align=center width=150px>
						<center>
							<span style="font-size:24px"><a href="https://scholar.google.com/citations?user=LUzvbGIAAAAJ&hl=en">Alessio Del Bue</a></span>
						</center>
					</td>
				</tr>
			</table>
			<br>
			<table align=center width=250px>
				<tr>
					<td align=center width=120px>
						<center>
							<span style="font-size:24px"><a href=''>[Paper]</a></span>
						</center>
					</td>
					<td align=center width=120px>
						<center>
							<span style="font-size:24px"><a href='https://github.com/IIT-PAVIS/MultimodalGaze'>[GitHub]</a></span><br>
						</center>
					</td>
				</tr>
			</table>
		</table>
	</center>

	<center>
		<table align=center width=850px>
			<tr>
				<td width=400px>
					<center>
						<img class="round" style="width:500px" src="/img/resources/gaze/figDataset.png"/>
					</center>
				</td>
			</tr>
		</table>
		<br>
		<table align=center width=850px>
			<tr>
				<td>
					Some instances from the dataset we curated using <a href="https://www.tobii.com/products/eye-trackers/wearables/tobii-pro-glasses-3">Tobii Eye Tracker</a>. Each row belongs to a single interaction session, while the frames (from left to right) are given in order of occurence. The dataset consists of multi scenarios consisting of different lightning condition, indoor/ outdoor scenes and occulsions. 
				</td>
			</tr>
		</table>
	</center>

	<hr>

	<table align=center width=850px>
		<center><h1>Abstract</h1></center>
		<tr>
			<td>
				Gaze prediction in egocentric videos is a fairly new research topic, which might have several applications for assistive technology (e.g., supporting blind people in their daily interactions), security (e.g., attention tracking in risky work environments), education (e.g., augmented / mixed reality training simulators, immersive games) and so forth. Egocentric gaze is typically estimated from video while few works attempt to use inertial measurement unit (IMU) data, a sensor modality often available in wearable devices (e.g., augmented reality headsets). Instead, in this paper, we examine whether joint learning of egocentric video and corresponding IMU data can improve the first-person gaze prediction compared to using these modalities separately. In this respect, we propose a multimodal network and evaluate it on several unconstrained social interaction scenarios captured by a first-person perspective. The proposed multimodal network achieves better results compared to unimodal methods as well as several (multimodal) baselines, showing that using egocentric video together with IMU data can boost the first-person gaze estimation performance.
			</td>
		</tr>
	</table>
	<br>

	<hr>
	<center><h1>Talk</h1></center>
	<p align="center">
		<iframe src="https://drive.google.com/file/d/1UexFQPMbeXwjlgECm1ggASlnyQtKt4t1/preview" width="660" height="395" allow="autoplay"></iframe>
	</p>

	<!-- <center>
	<div class="swipe-container">
		<div class="swipe-content">
			<div class="swipe-item">
				<iframe src="https://drive.google.com/file/d/1UexFQPMbeXwjlgECm1ggASlnyQtKt4t1/preview" width="660" height="395" allow="autoplay"></iframe>
			</div>
			<div class="swipe-item">
				<iframe src="https://docs.google.com/presentation/d/e/2PACX-1vQLzHYzQzWkty1IK4R8ZgX4Zy2VYRWjWhF4GBl4v2XEOky1Fo8YgixHJQWQKnfezQ/embed?start=true&loop=false&delayms=2000" frameborder="0" width="660" height="395" allowfullscreen="true" mozallowfullscreen="true" webkitallowfullscreen="true"></iframe>
			</div>
		</div>

		<div class="swipe-buttons">
			<div class="swipe-btn swipe-btn-left">&lt;</div>
			<div class="swipe-btn swipe-btn-right">&gt;</div>
		</div>
	</div>
	</center>
	<script>
		const container = document.querySelector('.swipe-container');
		const content = document.querySelector('.swipe-content');
		const leftBtn = document.querySelector('.swipe-btn-left');
		const rightBtn = document.querySelector('.swipe-btn-right');
		
		let currentX = 0;
		const itemWidth = container.offsetWidth; // Width of each swipe item
		
		leftBtn.addEventListener('click', swipeLeft);
		rightBtn.addEventListener('click', swipeRight);
		
		function swipeLeft() {
		  if (currentX >= 0) return; // Prevent swiping beyond the first item
		  currentX += itemWidth;
		  content.style.transform = `translateX(${currentX}px)`;
		}
		
		function swipeRight() {
		  const minTranslateX = -(content.offsetWidth - container.offsetWidth);
		  if (currentX <= minTranslateX) return; // Prevent swiping beyond the last item
		  currentX -= itemWidth;
		  content.style.transform = `translateX(${currentX}px)`;
		}
	  </script> -->


	<hr>

	<center><h1>Code</h1></center>

	<table align=center width=420px>
		<center>
			<tr>
				<td>
				</td>
			</tr>
		</center>
	</table>
	<table align=center width=400px>
		<tr>
			<td align=center width=300px>
				<center>
					<td><img class="round" style="width:750px" src="/img/resources/gaze/propmethod.png"/></td>
				</center>
			</td>
			<!-- <td align=center width=100px>
				<center>
					<td><img class="round" style="width:200px" src="/img/resources/gaze/propmethod.png"/></td>
				</center>
			</td> -->
		</tr>
	</table>
	<table align=center width=850px>
		<center>
			<tr>
				<td>
					Our (a) optical flow+IMU and (b) RGB+IMU multimodal networks for first-person gaze prediction. We regress
					(ùë•, ùë¶) gaze image positions by joint learning of two modalities.
					FC and BN stand for fully connected layer and batch normal-
					ization, respectively
				</td>
			</tr>
		</center>
	</table>
	<table align=center width=800px>
		<br>
		<tr><center>
			<span style="font-size:28px">&nbsp;<a href='https://github.com/IIT-PAVIS/MultimodalGaze'>[GitHub]</a>
			</center>
		</span>
	</table>
	<br>
	<hr>
	<center><h1>Results</h1></center>
	<table align=center width=420px>
		<center>
			<tr>
				<td>
				</td>
			</tr>
		</center>
	</table>
	<table align=center width=400px>
		<tr>
			<td align=center width=300px>
				<center>
					<td><img class="round" style="width:750px" src="/img/resources/gaze/result.png"/></td>
				</center>
			</td>
		</tr>
	</table>
	<table align=center width=850px>
		<center>
			<tr>
				<td>
					Results of our unimodal encoders (OF, RGB, IMU),
					our multimodal networks (OF+IMU, RGB+IMU) and baseline
					methods (center bias (CB), late fusion (LF) and joint learning
					with averaging (JA)) in terms of Accuracy (%). It can be noted that the multimodal learning generally tends to improve performance
					on the regression of gaze points. The improvement is majorily observed for RGB + IMU encoder when trained together provide better
					results as compared to individual encoders on different scenarios when tested using leave-one-out cross validation.
				</td>
			</tr>
		</center>
	</table>

	<p align="center">
		<iframe src="https://drive.google.com/file/d/1dCYfmwWh2OZhHD20sNhHmT5KEuGNBxlL/preview" width="660" height="395"  allow="autoplay"></iframe>

	</p>
	<table align=center width=850px>
			<td>
				The <span class="green-text"><b>green</b></span> dot represent the predicted gaze point from our multimodal encoder (RGB + IMU), and the <span class="red-text"><b>red</b> </span> dot point represent the ground-truth gaze point 
				from Tobii eye tracker. We propose using a box around the centered point to provide a vicinity around the center. A correct prediction is 
				classified if the region of overlap (IoU) between the ground-truth bouding box and predicted bouding box >= 0.5. 

				As can be seen from the result video, the predicted points are quite close to the ground-truth coordinates, although the instability is caused due to high contrast in the scene.
			</td>
	</table>
	<hr>
	<table align=center width=450px>
		<center><h1>Paper and Supplementary Material</h1></center>
		<tr>
			<td><a href=""><img class="layered-paper-big" style="height:175px" src="/img/resources/gaze/paper.png"/></a></td>
			<td><span style="font-size:14pt">S. Thakur, C. Beyan, P. Morerio, A. Del Bue<br>
				<b>Predicting Gaze from Egocentric Social Interaction Videos and IMU Data</b><br>
				In International Conference on Multimodal Interaction, 2021<br>
				(hosted on <a href="https://dl.acm.org/doi/abs/10.1145/3462244.3479954">ACM Digital</a>)<br>
				<!-- (<a href="./resources/camera-ready.pdf">camera ready</a>)<br> -->
				<span style="font-size:12pt"><a href=""><br></a>
					Mail:  <a href="mailto:sanket.thakur@iit.it">sanket.thakur@iit.it</a> for access to the data. 
				</span>
			</td>
		</tr>
	</table>
	<br>

	<table align=center width=600px>
		<tr>
			<td><span style="font-size:14pt"><center>
				<a href="/img/resources/gaze/bibtex.txt">[Bibtex]</a>
			</center></td>
		</tr>
	</table>

	<hr>
	<br>

	<table align=center width=900px>
		<tr>
			<td width=400px>
				<left>
					<center><h1>Acknowledgements</h1></center>
					This template was originally made by <a href="http://web.mit.edu/phillipi/">Phillip Isola</a> and <a href="http://richzhang.github.io/">Richard Zhang</a> for a <a href="http://richzhang.github.io/colorization/">colorful</a> ECCV project; the code can be found <a href="https://github.com/richzhang/webpage-template">here</a>.
				</left>
			</td>
		</tr>
	</table>

<br>
</body>
</html>

