<script src="http://www.google.com/jsapi" type="text/javascript"></script> 
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif; 
		font-weight:300;
		font-size:18px;
		margin-left: auto;
		margin-right: auto;
		width: 1100px;
	}
	
	h1 {
		font-size:32px;
		font-weight:300;
	}
	
	.disclaimerbox {
		background-color: #eee;		
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.rounded {
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}
	
	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}
	
	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		15px 15px 0 0px #fff, /* The fourth layer */
		15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		20px 20px 0 0px #fff, /* The fifth layer */
		20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		25px 25px 0 0px #fff, /* The fifth layer */
		25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}

	.paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35); /* The top layer shadow */

		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}

	.green-text{
		color: green;
	}
	.red-text{
		color: red;
	}
	.cyan-text{
		color: rgb(31, 134, 160);
	}
	
	.vert-cent {
		position: relative;
		top: 50%;
		transform: translateY(-50%);
	}
	
	hr
	{
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}

	.mySlides {
	display: none
	}
	img {
	vertical-align: middle;
	}
	.slideshow-container {
	max-width: 1000px;
	position: relative;
	margin: auto;
	}
	/* Next & previous buttons */
	.prev,
	.next {
	cursor: pointer;
	position: absolute;
	top: 50%;
	width: auto;
	padding: 16px;
	margin-top: -22px;
	color: white;
	font-weight: bold;
	font-size: 18px;
	transition: 0.6s ease;
	border-radius: 0 3px 3px 0;
	user-select: none;
	}
	/* Position the "next button" to the right */
	.next {
	right: 0;
	border-radius: 3px 0 0 3px;
	}
	/* On hover, add a black background color with a little bit see-through */
	.prev:hover,
	.next:hover {
	background-color: rgba(0, 0, 0, 0.8);
	}
	/* Caption text */
	.text {
	color: #0b0c0c;
	font-size: 15px;
	padding: 8px 12px;
	position: absolute;
	bottom: 8px;
	width: 100%;
	text-align: center;
	}
	/* Number text (1/3 etc) */
	.numbertext {
	color: #ffffff;
	font-size: 12px;
	padding: 8px 12px;
	position: absolute;
	top: 0;
	}
	/* The dots/bullets/indicators */
	.dot {
	cursor: pointer;
	height: 15px;
	width: 15px;
	margin: 0 2px;
	background-color: #999999;
	border-radius: 50%;
	display: inline-block;
	transition: background-color 0.6s ease;
	}
	.active,
	.dot:hover {
	background-color: #111111;
	}
	/* Fading animation */
	.fade {
	-webkit-animation-name: fade;
	-webkit-animation-duration: 1.5s;
	animation-name: fade;
	animation-duration: 1.5s;
	}
	@-webkit-keyframes fade {
	from {
		opacity: .4
	}
	to {
		opacity: 1
	}
	}
	@keyframes fade {
	from {
		opacity: .4
	}
	to {
		opacity: 1
	}
	}
	/* On smaller screens, decrease text size */
	@media only screen and (max-width: 300px) {
	.prev,
	.next,
	.text {
		font-size: 11px
	}
	}

	sub {
		vertical-align: sub;
		font-size: smaller;
	}
    

</style>

<html>
<head>
	<title>Enchancing Next-Active-Object based Egocentric Action Anticipation with Guided Attention</title>
	<meta property="og:image" content="Path to my teaser.png"/> <!-- Facebook automatically scrapes this. Go to https://developers.facebook.com/tools/debug/ if you update and want to force Facebook to rescrape. -->
	<meta property="og:title" content="Enchancing Next-Active-Object based Egocentric Action Anticipation with Guided Attention" />
	<meta property="og:description" content="This is the paper desc" />

	<!-- Get from Google Analytics -->
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src=""></script> 
	<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());

		gtag('config', 'UA-75863369-6');
	</script>
</head>

<body>
	<br>
	<center>
		<span style="font-size:34px">Enchancing Next-Active-Object based Egocentric Action Anticipation with Guided Attention</span><br>
		<br>
		<table align=center width=1000px>
			<table align=center width=1000px>
				<tr>
					<td align=center width=150px>
						<center>
							<span style="font-size:24px"><a href="https://sanketsans.github.io/about">Sanket Thakur</a></span>
						</center>
					</td>
					<td align=center width=150px>
						<center>
							<span style="font-size:24px"><a href="https://cbeyan.github.io/">Cigdem Beyan</a></span>
						</center>
					</td>
					<td align=center width=150px>
						<center>
							<span style="font-size:24px"><a href="https://scholar.google.it/citations?user=lPV9rbkAAAAJ&hl=it">Pietro Morerio</a></span>
						</center>
					</td>
					<td align=center width=150px>
						<center>
							<span style="font-size:24px"><a href="https://scholar.google.com/citations?user=yV3_PTkAAAAJ&hl=en">Vittorio Murino</a></span>
						</center>
					</td>
					<td align=center width=150px>
						<center>
							<span style="font-size:24px"><a href="https://scholar.google.com/citations?user=LUzvbGIAAAAJ&hl=en">Alessio Del Bue</a></span>
						</center>
					</td>
				</tr>
			</table>
			<br>
			<table align=center width=250px>
				<tr>
					<td align=center width=120px>
						<center>
							<span style="font-size:24px"><a href='https://arxiv.org/abs/2302.06358'>[Paper]</a></span>
						</center>
					</td>
					<td align=center width=300px>
						<center>
							<span style="font-size:18px"><a href='https://github.com/sanketsans/ganov2'>[GitHub]</a></span><br>
						</center>
					</td>
				</tr>
			</table>
		</table>
	</center>
	<br>
	<center>
		<table align=center width=850px>
			<tr>
				<td width=400px>
					<center>
						<img class="round" style="width:600px" src="/img/resources/guided_attn/icip23_method.gif"/>
					</center>
				</td>
			</tr>
		</table>
		<br>
		<table align=center width=850px>
			<tr>
				<td>
					The proposed Guided Attention fusion mechanism aims to combine th object detections and the spatial-temporal 
					features from videos to create better feature representation of video frames with enchanced contextual 
					information. The ''attended'' feature representation provides better results as compared to simple fusion
					based method. <br>
					Also, read about our <a href="https://arxiv.org/abs/2305.16066">Winner submission based on Guided-Attention for CVPR23 EGO4D STA challenge.</a>
				</td>
			</tr>
		</table>
	</center>

	<hr>

	<table align=center width=850px>
		<center><h1>Abstract</h1></center>
		<tr>
			<td>
				Short-term action anticipation (STA) in first-person videos is a challenging task that involves understanding 
				the next active object interactions and predicting future actions. Exist- ing action anticipation methods have 
				primarily focused on utilizing features extracted from video clips, but often over- looked the importance of objects 
				and their interactions. To this end, we propose a novel approach that applies a guided at- tention mechanism between 
				the objects, and the spatiotempo- ral features extracted from video clips, enhancing the motion and contextual 
				information, and further decoding the object- centric and motion-centric information to address the problem of STA 
				in egocentric videos. Our method, GANO (Guided Attention for Next active Objects) is a multi-modal, end-to- end, single 
				transformer-based network. The experimental re- sults performed on the largest egocentric dataset demonstrate that GANO 
				outperforms the existing state-of-the-art methods for the prediction of the next active object label, its bounding box 
				location, the corresponding future action, and the time to contact the object. The ablation study shows the positive 
				contribution of the guided attention mechanism compared to other fusion methods. Moreover, it is possible to improve 
				the next active object location and class label prediction results of GANO by just appending the learnable object tokens 
				with the region of interest embeddings.
			</td>
		</tr>
	</table>
	<br>

	<hr>

	<center><h1>Code</h1></center>

	<table align=center width=500px>
		<tr>
			<td align=center width=400px>
				<center>
					<td><img class="round" style="width:850px" src="/img/resources/guided_attn/icip23_method.png"/></td>
				</center>
			</td>
		</tr>
	</table>
	<br>
	<table align=center width=850px>
		<center>
			<tr>
				<td>
					Our GANO model uses a 3D convolution layer to extract frame patch features and an object detector to extract object
					embeddings from corresponding frames. These features are then fused together using a guided attention network to generate
					attended object-patch features. The attended features, F<sub>i</sub>, are then given to a transformer encoder layer, along with positional
					encoding, to obtain features (F<sub>e</sub>) from the last encoder layer. F<sub>e</sub> are then used to extract Regions of Interest (ROIs) from the
					last observed frame, which are used to predict future actions and Time to Contact (TTC) (ˆv and δ, respectively) for the detected
					objects. Additionally, we append the learnable tokens to the ROI embeddings, creating a fixed query length, and use them to
					generate the next active object-related prediction
				</td>
			</tr>
		</center>
	</table>
	<table align=center width=800px>
		<br>
		<tr><center>
			<span style="font-size:28px">&nbsp;<a href='https://github.com/sanketsans/ganov2'>[GitHub]</a>
			</center>
		</span>
	</table>
	<br>

	<br>
	<hr>
	<table align=center width=450px>
		<center><h1>Paper and Supplementary Material</h1></center>
		<tr>
			<td><a href=""><img class="layered-paper-big" style="height:175px" src="/img/resources/guided_attn/paper.png"/></a></td>
			<td><span style="font-size:14pt">S. Thakur, C. Beyan, P. Morerio, V. Murino, A. Del Bue<br>
				<b>Enchancing Next-Active-Object based Egocentric Action Anticipation with Guided Attention</b><br>
				<!-- In International Conference on Multimodal Interaction, 2021<br> -->
				(hosted on <a href="https://arxiv.org/abs/2305.12953">arxiv</a>)<br>
				<!-- (<a href="./resources/camera-ready.pdf">camera ready</a>)<br> -->
				Also, read about our <a href="https://arxiv.org/abs/2305.16066">Winner submission based on Guided-Attention for CVPR23 EGO4D STA challenge.</a>
			</td>
		</tr>
	</table>
	<br>

	<table align=center width=600px>
		<tr>
			<td><span style="font-size:14pt"><center>
				<a href="/img/resources/nao_journal/bibtex.txt">[Bibtex]</a>
			</center></td>
		</tr>
	</table>

	<hr>
	<br>

	<table align=center width=900px>
		<tr>
			<td width=400px>
				<left>
					<center><h1>Acknowledgements</h1></center>
					This template was originally made by <a href="http://web.mit.edu/phillipi/">Phillip Isola</a> and <a href="http://richzhang.github.io/">Richard Zhang</a> for a <a href="http://richzhang.github.io/colorization/">colorful</a> ECCV project; the code can be found <a href="https://github.com/richzhang/webpage-template">here</a>.
				</left>
			</td>
		</tr>
	</table>

<br>
</body>
</html>

