<script src="http://www.google.com/jsapi" type="text/javascript"></script> 
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif; 
		font-weight:300;
		font-size:18px;
		margin-left: auto;
		margin-right: auto;
		width: 1100px;
	}
	
	h1 {
		font-size:32px;
		font-weight:300;
	}
	
	.disclaimerbox {
		background-color: #eee;		
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.rounded {
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}
	
	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}
	
	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		15px 15px 0 0px #fff, /* The fourth layer */
		15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		20px 20px 0 0px #fff, /* The fifth layer */
		20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		25px 25px 0 0px #fff, /* The fifth layer */
		25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}

	.paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35); /* The top layer shadow */

		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}

	.green-text{
		color: green;
	}
	.red-text{
		color: red;
	}
	.cyan-text{
		color: rgb(31, 134, 160);
	}
	
	.vert-cent {
		position: relative;
		top: 50%;
		transform: translateY(-50%);
	}
	
	hr
	{
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}

	.mySlides {
	display: none
	}
	img {
	vertical-align: middle;
	}
	.slideshow-container {
	max-width: 1000px;
	position: relative;
	margin: auto;
	}
	/* Next & previous buttons */
	.prev,
	.next {
	cursor: pointer;
	position: absolute;
	top: 50%;
	width: auto;
	padding: 16px;
	margin-top: -22px;
	color: white;
	font-weight: bold;
	font-size: 18px;
	transition: 0.6s ease;
	border-radius: 0 3px 3px 0;
	user-select: none;
	}
	/* Position the "next button" to the right */
	.next {
	right: 0;
	border-radius: 3px 0 0 3px;
	}
	/* On hover, add a black background color with a little bit see-through */
	.prev:hover,
	.next:hover {
	background-color: rgba(0, 0, 0, 0.8);
	}
	/* Caption text */
	.text {
	color: #0b0c0c;
	font-size: 15px;
	padding: 8px 12px;
	position: absolute;
	bottom: 8px;
	width: 100%;
	text-align: center;
	}
	/* Number text (1/3 etc) */
	.numbertext {
	color: #ffffff;
	font-size: 12px;
	padding: 8px 12px;
	position: absolute;
	top: 0;
	}
	/* The dots/bullets/indicators */
	.dot {
	cursor: pointer;
	height: 15px;
	width: 15px;
	margin: 0 2px;
	background-color: #999999;
	border-radius: 50%;
	display: inline-block;
	transition: background-color 0.6s ease;
	}
	.active,
	.dot:hover {
	background-color: #111111;
	}
	/* Fading animation */
	.fade {
	-webkit-animation-name: fade;
	-webkit-animation-duration: 1.5s;
	animation-name: fade;
	animation-duration: 1.5s;
	}
	@-webkit-keyframes fade {
	from {
		opacity: .4
	}
	to {
		opacity: 1
	}
	}
	@keyframes fade {
	from {
		opacity: .4
	}
	to {
		opacity: 1
	}
	}
	/* On smaller screens, decrease text size */
	@media only screen and (max-width: 300px) {
	.prev,
	.next,
	.text {
		font-size: 11px
	}
	}

	sub {
		vertical-align: sub;
		font-size: smaller;
	}
    

</style>

<html>
<head>
	<title>Anticipating Next Active Object for egocentric videos</title>
	<meta property="og:image" content="Path to my teaser.png"/> <!-- Facebook automatically scrapes this. Go to https://developers.facebook.com/tools/debug/ if you update and want to force Facebook to rescrape. -->
	<meta property="og:title" content="Anticipating Next Active Object for egocentric videos" />
	<meta property="og:description" content="This is the paper desc" />

	<!-- Get from Google Analytics -->
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src=""></script> 
	<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());

		gtag('config', 'UA-75863369-6');
	</script>
</head>

<body>
	<br>
	<center>
		<span style="font-size:34px">Anticipating Next Active Object for egocentric videos</span><br>
		<br>
		<table align=center width=1000px>
			<table align=center width=1000px>
				<tr>
					<td align=center width=150px>
						<center>
							<span style="font-size:24px"><a href="https://sanketsans.github.io/about">Sanket Thakur</a></span>
						</center>
					</td>
					<td align=center width=150px>
						<center>
							<span style="font-size:24px"><a href="https://cbeyan.github.io/">Cigdem Beyan</a></span>
						</center>
					</td>
					<td align=center width=150px>
						<center>
							<span style="font-size:24px"><a href="https://scholar.google.it/citations?user=lPV9rbkAAAAJ&hl=it">Pietro Morerio</a></span>
						</center>
					</td>
					<td align=center width=150px>
						<center>
							<span style="font-size:24px"><a href="https://scholar.google.com/citations?user=yV3_PTkAAAAJ&hl=en">Vittorio Murino</a></span>
						</center>
					</td>
					<td align=center width=150px>
						<center>
							<span style="font-size:24px"><a href="https://scholar.google.com/citations?user=LUzvbGIAAAAJ&hl=en">Alessio Del Bue</a></span>
						</center>
					</td>
				</tr>
			</table>
			<br>
			<table align=center width=250px>
				<tr>
					<td align=center width=120px>
						<center>
							<span style="font-size:24px"><a href='https://arxiv.org/abs/2302.06358'>[Paper]</a></span>
						</center>
					</td>
					<td align=center width=300px>
						<center>
							<span style="font-size:18px">[GitHub] (Soon)</span><br>
						</center>
					</td>
				</tr>
			</table>
		</table>
	</center>
	<br>
	<center>
		<table align=center width=850px>
			<tr>
				<td width=400px>
					<center>
						<img class="round" style="width:600px" src="/img/resources/nao_journal/wacv_teas.png"/>
					</center>
				</td>
			</tr>
		</table>
		<br>
		<table align=center width=850px>
			<tr>
				<td>
					The next-active-object (NAO) problem formulation in our paper is inspired from action anticipation based setup. Let 	<b><i>V</i></b>, be a given video clip, 
					we split the video clip into three sequential parts: the observed segment of length &#x3C4;<sub>o</sub>, the time to contact (TTC) window
					of length &#x3C4;<sub>a</sub> and a given action segment which starts at timestep <it>t = </it> &#x3C4;<sub>s</sub>. The goal is to observe a video segment till
					&#x3C4;<sub>a</sub> and localize NAO at the beginning of an action segment at timestep <it>t = </it>	&#x3C4;<sub>s</sub>, where contact might happen.
				</td>
			</tr>
		</table>
	</center>

	<hr>

	<table align=center width=850px>
		<center><h1>Abstract</h1></center>
		<tr>
			<td>
				This paper addresses the problem of anticipating the next-active-object location
				in the future, for a given egocentric video clip where the contact might happen,
				before any action takes place. The problem is considerably hard, as we aim at
				estimating the position of such objects in a scenario where the observed clip and
				the action segment are separated by the so-called time to contact segment. We
				name this task Anticipating the Next ACTive Object (ANACTO). To this end, we
				propose a transformer-based self-attention framework to identify and locate the
				next-active-object in an egocentric clip, where the contact might happen to un-
				dertake a human-object interaction. We benchmark our method on three major
				egocentric datasets namely, EpicKitchen-100, EGTEA+ and Ego4D. Since the defined task is new, we compare our model with
				the state-of-the-art action anticipation-based method(s) to curate relevant base-
				line methods. In the end, we also provide the annotations for next-active-object for EpicKitchen-100 and EGETA+ datasets.
			</td>
		</tr>
	</table>
	<br>

	<hr>

	<center><h1>Code</h1></center>

	<table align=center width=400px>
		<tr>
			<td align=center width=300px>
				<center>
					<td><img class="round" style="width:850px" src="/img/resources/nao_journal/method.png"/></td>
				</center>
			</td>
		</tr>
	</table>
	<br>
	<table align=center width=850px>
		<center>
			<tr>
				<td>
					Our T-ANACTO model is an encoder-decoder architecture. Its encoder is composed of
					an object detector and a Vision Transformer (ViT). The object detector takes an input frame
					(e.g., size of 1920×1080) and predicts the location of objects in terms of bounding boxes (x, y,
					w, h) and detection confidence scores (c). ViT takes the resized (224x224) frames as input,
					and then divide it into the patches (16×16). The object detections are also
					converted to match the scaled size of the frame (i.e., 224×224), reshaped, and are then passed
					through a MLP to convert it into the same dimension as the embeddings from the transformer
					encoder, which are later concatenated together to be given to the decoder. Transformer decoder uses temporal aggregation to predict the
					next active object. For each frame, the decoder aggregate the features from the encoder for current
					and past frames along with the embeddings of last predicted active objects and then predicts the
					next active object for the future frames.
				</td>
			</tr>
		</center>
	</table>
	<table align=center width=800px>
		<br>
		<tr><center>
			<span style="font-size:28px">&nbsp;[GitHub] (To be Released soon)
			</center>
		</span>
	</table>
	<br>
	<hr>
	<center><h1>ANACTO annotations</h1></center>
	<table align=center width=850px>
		<td>
			The annotations for EpicKitchen-100 and EGTEA+ dataset for ANACTO task was curated using an Hand-Object detector <span class="cyan-text"><a href="https://fouheylab.eecs.umich.edu/~dandans/projects/100DOH/">Shan et al.</a></span>. 
			The detector is currently SOTA on EpicKitchen-100. We used the Hand-Object detector and ran it on both the datasets (EpicKitchen-100 and EGTEA+). The annotations (bounding boxes - left hand,
			right hand, active object(s)) are extracted for each frame. We used the action anticipation splits from <span class="cyan-text"><a href="https://iplab.dmi.unict.it/rulstm/">Furnari et al.</a></span> to label the 
			next-active-object annotations at the beginning of an action. <br>
			We provide the annotations for 94% and 92% of video clips in EpicKitchen-100 and EGTEA+ dataset respectively. <br>
			<br>
			It is to be noted that the T-ANACTO model is designed for identifying the next-active-object location at the beginning of an action, after &#x3C4;<sub>a</sub>. However, based on recent literature 
				<span class="cyan-text"><a href="https://ego4d-data.org/">Grauman et al.</a></span>, for the need of identifying next-active-object in the last observed frame, we provide 
				relevant models' visualizations to argue that T-ANACTO is also able to locate its attention to important regions in the last observed frame to predict the next-active-object. 
			
		</td>
	</table>
	<br>
	<hr>

	<center><h1>Results</h1></center>

	<p align="center">
		<iframe src="https://drive.google.com/file/d/1hXZ42A6a2Aoi0mUg3a6xD7sdof5Tjsuv/preview" width="640" height="480" allow="autoplay">
		</iframe>
	</p>

	<table align=center width=850px>
			<td>
				We provide the visualizations of the attention of model over multiple video clips. We randomly stop at a random frame in the observed segment to provide
				the effective visualizations of our model, T-ANACTO, to identify important regions in the frame based on past motion. Based on the heatmaps, it can be 
				identified that the model is also capable in anticipating the next-active-object in the last observed frame as well.  
				The <span class="green-text">green</span> bounding boxes are the predicted bounding boxes by T-ANACTO at the beginning of an action. 
			</td>
	</table>
	<hr>
	<center><h1>Model Visualizations</h1></center>
	<center>
		<table align=center width=850px>
			<td>
				The pictures presented here are the attention visualization of the model <it>w.r.t</it> last observed frame of an observed segment. 
			</td>
		</table>
		<div class="slideshow-container">
			<div class="mySlides fade">
			  <div class="numbertext">1 / 7</div>
			  <img src="/img/resources/nao_journal/wacv23_epic0.png" style="width:100%">
			  <figcaption >Results on <b><it>EpicKitchen-100</it></b> for last observed frame of video clip with TTC
				&#x3C4;<sub>a</sub> = 0.5 seconds before the beginning of an action.</figcaption>
			</div>
			<div class="mySlides fade">
			  <div class="numbertext">2 / 7</div>
			  <img src="/img/resources/nao_journal/wacv23_ego_6.png" style="width:100%">
			  <figcaption>Results for <b><it>Ego4D</it></b>
				dataset when trained to identify next active object wrt last ob-
				served frame. </figcaption>
			</div>
			<div class="mySlides fade">
			  <div class="numbertext">3 / 7</div>
			  <img src="/img/resources/nao_journal/wacv23_gtea.png" style="width:100%">
			  <figcaption>Results on <b><it>EGTEA+</it></b> for last observed frame of video clip with 
				&#x3C4;<sub>a</sub> = 1.0 seconds before the beginning of an action.</figcaption>
			</div>
			<div class="mySlides fade">
				<div class="numbertext">4 / 7</div>
				<img src="/img/resources/nao_journal/wacv23_comp.png" style="width:100%">
				<figcaption>Results show the diversity of spatial attention for the last observed frame preceding
					the beginning of an action segment for different setups of TTC window for &#x3C4;<sub>a</sub>= 0.25, 0.5, 1.0
					seconds. The spatial attention regions tend to appear more assertive as the model examines the
					frames closer to an action segment, i.e; as the τa is decreased</figcaption>
			</div>
			<div class="mySlides fade">
				<div class="numbertext">5 / 7</div>
				<img src="/img/resources/nao_journal/eccv_hands_detec.png" style="width:100%">
				<figcaption>Our T-ANACTO model also learns
					to attribute to hand position in an image frame, even though we do not explicitly provide the hand
					location in training and testing. </figcaption>
			</div>
			<div class="mySlides fade">
				<div class="numbertext">6 / 7</div>
				<img src="/img/resources/nao_journal/eccv_fail_glass.png" style="width:100%">
				<figcaption>The model fails
					to attribute attention to objects which are light colored or easily
					camouflaged with the background. </figcaption>
			</div>
			<div class="mySlides fade">
				<div class="numbertext">7 / 7</div>
				<img src="/img/resources/nao_journal/eccv_fail_time.png" style="width:100%">
				<figcaption>When the scene completely
					changes at the beginning of action from the past observed segment.</figcaption>
			</div>
			<a class="prev" onclick="plusSlides(-1)">&#10094;</a>
			<a class="next" onclick="plusSlides(1)">&#10095;</a>
		  </div>
		  <br>
		  <div style="text-align:center">
			<span class="dot" onclick="currentSlide(0)"></span>
			<span class="dot" onclick="currentSlide(1)"></span>
			<span class="dot" onclick="currentSlide(2)"></span>
			<span class="dot" onclick="currentSlide(3)"></span>
			<span class="dot" onclick="currentSlide(4)"></span>
			<span class="dot" onclick="currentSlide(5)"></span>
			<span class="dot" onclick="currentSlide(6)"></span>
			<span class="dot" onclick="currentSlide(7)"></span>
		  </div>
		</center>
		
		<script>
			let slideIndex = 0;
			let timeoutId = null;
			const slides = document.getElementsByClassName("mySlides");
			const dots = document.getElementsByClassName("dot");
			
			showSlides();
			function currentSlide(index) {
				slideIndex = index;
				showSlides();
			}
			function plusSlides(step) {
				
				if(step < 0) {
					slideIndex -= 2;
					
					if(slideIndex < 0) {
					slideIndex = slides.length - 1;
					}
				}
				
				showSlides();
			}
			function showSlides() {
				for(let i = 0; i < slides.length; i++) {
				slides[i].style.display = "none";
				dots[i].classList.remove('active');
				}
				slideIndex++;
				if(slideIndex > slides.length) {
				slideIndex = 1
				}
				slides[slideIndex - 1].style.display = "block";
				dots[slideIndex - 1].classList.add('active');
				if(timeoutId) {
					clearTimeout(timeoutId);
				}
				timeoutId = setTimeout(showSlides, 7000); // Change image every 5 seconds
			}
		</script>
	<br>
	<hr>
	<table align=center width=450px>
		<center><h1>Paper and Supplementary Material</h1></center>
		<tr>
			<td><a href=""><img class="layered-paper-big" style="height:175px" src="/img/resources/nao_journal/paper.png"/></a></td>
			<td><span style="font-size:14pt">S. Thakur, C. Beyan, P. Morerio, V. Murino, A. Del Bue<br>
				<b>Anticipating Next Active Objects for Egocentric Videos</b><br>
				<!-- In International Conference on Multimodal Interaction, 2021<br> -->
				(hosted on <a href="https://arxiv.org/abs/2302.06358">arxiv</a>)<br>
				<!-- (<a href="./resources/camera-ready.pdf">camera ready</a>)<br> -->
				<span style="font-size:12pt"><a href=""><br></a>
					Mail:  <a href="mailto:sanket.thakur@iit.it">sanket.thakur@iit.it</a> for access to the next-active-object data for EK-100 and EGTEA.
				</span>
			</td>
		</tr>
	</table>
	<br>

	<table align=center width=600px>
		<tr>
			<td><span style="font-size:14pt"><center>
				<a href="/img/resources/nao_journal/bibtex.txt">[Bibtex]</a>
			</center></td>
		</tr>
	</table>

	<hr>
	<br>

	<table align=center width=900px>
		<tr>
			<td width=400px>
				<left>
					<center><h1>Acknowledgements</h1></center>
					This template was originally made by <a href="http://web.mit.edu/phillipi/">Phillip Isola</a> and <a href="http://richzhang.github.io/">Richard Zhang</a> for a <a href="http://richzhang.github.io/colorization/">colorful</a> ECCV project; the code can be found <a href="https://github.com/richzhang/webpage-template">here</a>.
				</left>
			</td>
		</tr>
	</table>

<br>
</body>
</html>

